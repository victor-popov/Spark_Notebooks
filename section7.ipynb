{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "java8_location= '/usr/lib/jvm/java-8-openjdk-amd64' # Set your own\n",
    "os.environ['JAVA_HOME'] = java8_location\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is some test data.',\n",
       " 'That takes more than one line.',\n",
       " 'This is a second test file.',\n",
       " 'Is it also',\n",
       " 'a multiline file.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('Working Files/07/sample_inputs').collect()\n",
    "# RDD with n elements where each element is a separate line of the txt file;\n",
    "# All files in the specified directory gets concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/victor/Desktop/Spark/Working Files/07/sample_inputs/test.txt',\n",
       "  'This is some test data.\\nThat takes more than one line.\\n'),\n",
       " ('file:/home/victor/Desktop/Spark/Working Files/07/sample_inputs/test2.txt',\n",
       "  'This is a second test file.\\nIs it also\\na multiline file.\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.wholeTextFiles('Working Files/07/sample_inputs').collect()\n",
    "# List of pairs, where first element of a tuple is a filename, and the second one is the whole content of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wholeTextFiles works best with the small files;\n",
    "\n",
    "textFile allows to better parallelize (as we can use more partitions);\n",
    "\n",
    "minPartitions optional argument can be specified;\n",
    "\n",
    "useUnicode optional argument can be specified; this can make processing slightly faster (but you should be sure, that your documents do not contain weird symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(['a', 1, {'some_key': 'some_value'}]).saveAsPickleFile('Working Files/07/pickled_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 1, {'some_key': 'some_value'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pickleFile('Working Files/07/pickled_file').collect()\n",
    "# we didn't get the strings or jsons or anything else, but we get a list of RDD objects, which do not require any\n",
    "# additional parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even classes can be pickled, but only those that are at the top level of the module that it is in.\n",
    "# Nested or dynamically created classes can not be pickled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more data we serialize the longer it takes to save and read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thrift can be used for better performance, but requires specifying schema beforehand. \n",
    "# Can be read from different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "org.apache.hadoop.mapred -> old API\n",
    "\n",
    "org.apache.hadoop.mapreduce -> new API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if old API:\n",
    "\n",
    "- sc.hadoopFile\n",
    "\n",
    "- sc.hadoopRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if new API:\n",
    "\n",
    "- sc.newAPIHadoopFile\n",
    "\n",
    "- sc.newAPIHadoopRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoopFile command is used when we provide a file to open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.newAPIHadoopFile(path='Working Files/07/test.txt',\n",
    "                          inputFormatClass='org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "                          keyClass='org.apache.hadoop.io.LongWritable',\n",
    "                          valueClass='org.apache.hadoop.io.Text',\n",
    "                          keyConverter=None,\n",
    "                          valueConverter=None,\n",
    "                          conf={},\n",
    "                          batchSize=0) # -> how many python objects should be represented as a single Java object\n",
    "                                       # 0 -> decision will be made automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'This is some test data.'), (24, 'That takes more than one line.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old API -> org.apache.hadoop.mapred\n",
    "\n",
    "- saveAsHadoopDataset\n",
    "- saveAsHadoopFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New API -> org.apache.hadoop.mapreduce\n",
    "\n",
    "- saveAsNewAPIHadoopDataset\n",
    "- saveAsNewAPIHadoopFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsHadoopDataset(conf, keyConverter, valueConverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsHadoopFile(path,\n",
    "                     outputFormatClass,  \n",
    "                     keyClass, \n",
    "                     valueClass,\n",
    "                     keyConverter,\n",
    "                     valueConverter,\n",
    "                     conf,\n",
    "                     compressionCodecClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
